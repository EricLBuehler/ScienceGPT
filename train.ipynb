{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BHZnXywjxGeq"
   },
   "outputs": [],
   "source": [
    "#https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kY_j8RUDP5y4"
   },
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import math\n",
    "from torch.autograd.variable import Variable\n",
    "import typing\n",
    "import random\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "y4lRdOSZNpe2"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ph2WO5JpKukk",
    "outputId": "ab00d895-307a-4ade-e705-1c6dd8ae6926"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "modelname=\"4_24_23_m1\"\n",
    "\n",
    "prefix_models=\"ScienceGPT/models/\"+modelname+\"/\"\n",
    "\n",
    "if not os.path.exists(prefix_models):\n",
    "    os.makedirs(prefix_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Oe3wtt5oADl6"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xGOoJdNdRSaO"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        #mine is [batch, seq, embed]\n",
    "        x = x.permute((1,0,2))\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        dropout = self.dropout(x)\n",
    "        return dropout.permute((1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XakHZ4HfFdYM"
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        key_tp = key.transpose(-2, -1)\n",
    "\n",
    "        scores = query.matmul(key_tp) / math.sqrt(query.size()[-1])\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "            \n",
    "        attention = F.softmax(scores, dim = -1)\n",
    "\n",
    "        return attention.matmul(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "G-sb6gd7L5bR"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 head_num,\n",
    "                 bias=True,\n",
    "                 activation=F.relu):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        if in_features % head_num != 0:\n",
    "            raise ValueError('`in_features`({}) should be divisible by \\\n",
    "                `head_num`({})'.format(in_features, head_num))\n",
    "        self.in_features = in_features\n",
    "        self.head_num = head_num\n",
    "        self.activation = activation\n",
    "        self.bias = bias\n",
    "        self.linear_q = nn.Linear(in_features, in_features, bias)\n",
    "        self.linear_k = nn.Linear(in_features, in_features, bias)\n",
    "        self.linear_v = nn.Linear(in_features, in_features, bias)\n",
    "        self.linear_o = nn.Linear(in_features, in_features, bias)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        q, k, v = self.linear_q(q), self.linear_k(k), self.linear_v(v)\n",
    "        if self.activation is not None:\n",
    "            q = self.activation(q)\n",
    "            k = self.activation(k)\n",
    "            v = self.activation(v)\n",
    "\n",
    "        q = self._reshape_to_batches(q)\n",
    "        k = self._reshape_to_batches(k)\n",
    "        v = self._reshape_to_batches(v)\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.repeat(self.head_num, 1, 1)   \n",
    "        \n",
    "        y = ScaledDotProductAttention()(q, k, v, mask)        \n",
    "        \n",
    "        y = self._reshape_from_batches(y)      \n",
    "\n",
    "        y = self.linear_o(y)\n",
    "        if self.activation is not None:\n",
    "            y = self.activation(y)\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def gen_causal_mask(x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        return torch.tril(torch.ones(seq_len, seq_len)).view(1, seq_len, seq_len).repeat(batch_size, 1, 1)\n",
    "\n",
    "    def _reshape_to_batches(self, x):\n",
    "        batch_size, seq_len, in_feature = x.size()\n",
    "        sub_dim = in_feature // self.head_num\n",
    "        return x.reshape(batch_size, seq_len, self.head_num, sub_dim)\\\n",
    "                .permute(0, 2, 1, 3)\\\n",
    "                .reshape(batch_size * self.head_num, seq_len, sub_dim)\n",
    "\n",
    "    def _reshape_from_batches(self, x):\n",
    "        batch_size, seq_len, in_feature = x.size()\n",
    "        batch_size //= self.head_num\n",
    "        out_dim = in_feature * self.head_num\n",
    "        return x.reshape(batch_size, self.head_num, seq_len, in_feature)\\\n",
    "                .permute(0, 2, 1, 3)\\\n",
    "                .reshape(batch_size, seq_len, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NDpTASiaTxTX"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_dim: int,\n",
    "                 n_self_heads: int,\n",
    "                 n_features: int,\n",
    "                 n_layers: int,\n",
    "                 n_classes: int):\n",
    "        super().__init__()\n",
    "\n",
    "        #Embedding layer\n",
    "        self.embedding = nn.Embedding(n_features, embedding_dim)\n",
    "        #Positional encoding\n",
    "        self.pos_encode = PositionalEncoding(embedding_dim)\n",
    "\n",
    "        self.decoder_layers = []\n",
    "\n",
    "        for _ in range(n_layers):\n",
    "            layer = []\n",
    "            #Add multihead, which will be cross or self attention\n",
    "            layer.append(MultiHeadAttention(embedding_dim, n_self_heads)) #self attention first, masked\n",
    "            #Now add layer norm\n",
    "            layer.append(nn.LayerNorm(embedding_dim))\n",
    "            #Add a feed forward\n",
    "            layer.append(nn.Linear(embedding_dim, embedding_dim))\n",
    "            #Now add layer norm\n",
    "            layer.append(nn.LayerNorm(embedding_dim))\n",
    "\n",
    "            self.decoder_layers.append(nn.ModuleList(layer))\n",
    "        self.decoder_layers=nn.ModuleList(self.decoder_layers)\n",
    "\n",
    "        self.to_out = nn.Linear(embedding_dim, n_classes)\n",
    "            \n",
    "    def forward(self, x: torch.Tensor, calculate_loss: bool = False):\n",
    "        \"\"\"\n",
    "        Expect tensor of [batch_size, n_features]\n",
    "        \"\"\"\n",
    "        if calculate_loss:\n",
    "            #If give model that accepts ?x?x4 abcd, expect bcd0\n",
    "            \n",
    "            target_logits=torch.cat([x[:,1:], torch.zeros((x.shape[0],1)).to(device)], dim=-1) ## if x is abcd, then target_logits is bcd0\n",
    "\n",
    "        x=x.long().to(device)\n",
    "        embed = self.embedding(x)\n",
    "        pos_encode = self.pos_encode(embed)\n",
    "\n",
    "        res = embed+pos_encode\n",
    "        \n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            d_self_attention = decoder_layer[0]\n",
    "            d_layer_norm_1 = decoder_layer[1]\n",
    "            d_ff = decoder_layer[2]\n",
    "            d_layer_norm_2 = decoder_layer[3]\n",
    "            \n",
    "            ## Run the decoder\n",
    "            #do masked self attention\n",
    "            mask = MultiHeadAttention.gen_causal_mask(res).to(device)\n",
    "            res = res + d_self_attention(res,res,res, mask = mask)\n",
    "            self_res = res\n",
    "            #layer norm\n",
    "            res = d_layer_norm_1(res)\n",
    "\n",
    "            #do ff\n",
    "            res = self_res + d_ff(res)\n",
    "            #layer norm\n",
    "            res = d_layer_norm_2(res)\n",
    "\n",
    "        out = self.to_out(res)\n",
    "        if calculate_loss:\n",
    "            loss = nn.functional.cross_entropy(out.permute(0, 2, 1), target_logits.long())\n",
    "            return out,loss\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HqGGEpvCLbP8"
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, n_pad: int, device: torch.device, pad_byte: int = 0, split: str = \"\\n\"):\n",
    "        self.n_pad = n_pad\n",
    "        self.device = device\n",
    "        self.pad_byte = pad_byte\n",
    "        self.split = split\n",
    "\n",
    "    def tokenize_str(self, sentence: str, encoding = \"utf8\") -> torch.Tensor:\n",
    "        base = [int(i) for i in bytes(sentence, encoding)]\n",
    "        if len(base) < self.n_pad:\n",
    "            base.extend([self.pad_byte] * (self.n_pad - len(base)))\n",
    "        assert len(base) == self.n_pad, f\"n_pad is too small, use {len(base)} or greater.\"\n",
    "        tensor = torch.Tensor(base)\n",
    "        return tensor.to(self.device)\n",
    "\n",
    "    def texts_to_sequences(self, texts: typing.List[str], encoding = \"utf8\") -> torch.Tensor:\n",
    "        # tokenize the input text\n",
    "        sentences = []\n",
    "        for sentence in texts:\n",
    "            sentences.append(self.tokenize_str(sentence).unsqueeze(0))\n",
    "\n",
    "        return torch.cat(sentences, dim = 0).to(self.device)\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_texts(document: str) -> typing.List[str]:\n",
    "        return filter(lambda x: len(x)!=0, document.split(self.split))\n",
    "    \n",
    "    def sequences_to_texts(self, texts: torch.Tensor, encoding = \"utf8\") -> typing.List[str]:\n",
    "        out = []\n",
    "        for seq in texts:\n",
    "            chars = []\n",
    "            i=0\n",
    "            while i<len(seq) and seq[i] != 0:\n",
    "                chars.append(int(seq[i]))\n",
    "                i+=1\n",
    "            out.append(bytes(chars).decode(encoding))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pB-Xcf4TkKNQ"
   },
   "outputs": [],
   "source": [
    "def generate(seed: str, cutoff: int = 128) -> str:\n",
    "    output = torch.tensor([list(bytes(seed,\"utf8\"))]).to(device)\n",
    "    \n",
    "    res=output\n",
    "    last = -1\n",
    "    i=0\n",
    "    while last != 0 and i<cutoff:\n",
    "        res = model(output)\n",
    "        argmax=res.argmax(-1)\n",
    "        \n",
    "        out = list(output[0])\n",
    "        out.append(list(argmax.to(device)[0])[-1])\n",
    "        last = list(argmax.to(device)[0])[-1]\n",
    "        output = torch.tensor([out])\n",
    "        i+=1\n",
    "    \n",
    "    if last == 0:\n",
    "        return convert_to_str(output)\n",
    "    return convert_to_str(output)+\"<CUTOFF>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "R81mAag8mHEG"
   },
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data: typing.List[str], n_pad):\n",
    "        self.raw_data = data\n",
    "        self.tokenizer = Tokenizer(n_pad, device, split = \"\\0\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.tokenizer.tokenize_str(self.raw_data[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xQidsLjVaFUi"
   },
   "outputs": [],
   "source": [
    "def convert_to_str(x: torch.Tensor) -> str:\n",
    "    #Expects [1, 256] tensor\n",
    "    bts = []\n",
    "    i=0\n",
    "    while len(bts)<x.shape[1] and x[0][i] != 0:\n",
    "        bts.append(int(x[0][i]))\n",
    "        i+=1\n",
    "    return bytes(bts).decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "97rbDfS4gO8C"
   },
   "outputs": [],
   "source": [
    "n_features = 256 # No. of tokens\n",
    "n_pad = 2048 # Max line length\n",
    "embedding_dim = 256\n",
    "train_split = 0.9\n",
    "batch_size = 16\n",
    "head_factor = 64\n",
    "assert embedding_dim%head_factor == 0\n",
    "head_size = embedding_dim//head_factor\n",
    "n_layers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IvvenMeUKfaj",
    "outputId": "fb38b6f5-0eaf-4d89-83ad-bee41604b8ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13345\n"
     ]
    }
   ],
   "source": [
    "path_to_data = \"data/reddit_scrape.txt\"\n",
    "data_raw = open(path_to_data, encoding=\"utf-8\").read()\n",
    "\n",
    "data_split = list(filter(lambda x: x!=\"\", data_raw.split(\"\\0\")))\n",
    "random.shuffle(data_split)\n",
    "\n",
    "n = int(train_split * len(data_split))\n",
    "print(n)\n",
    "train_data = data_split[:n]\n",
    "val_data = data_split[n:]\n",
    "\n",
    "train_dataloader = TextDataset(train_data, n_pad)\n",
    "test_dataloader = TextDataset(train_data, n_pad)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(train_dataloader, batch_size=batch_size)\n",
    "testloader = torch.utils.data.DataLoader(test_dataloader , batch_size=1)\n",
    "testloader_iter = iter(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "GvibftHanb0A"
   },
   "outputs": [],
   "source": [
    "model = Transformer(embedding_dim, head_size, n_features, n_layers, 256)\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PXqmUSK0M3hD",
    "outputId": "69ad8160-d138-4e98-99b6-bd8cb83753c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048])\n",
      "torch.Size([1, 2048, 256])\n",
      "tensor([[ 72,  77,  59,  ..., 181, 181, 181]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "input=next(testloader_iter)\n",
    "input=input.to(device)\n",
    "res = model(input)\n",
    "print(input.shape)\n",
    "print(res.shape)\n",
    "print(res.argmax(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "tZji9Va7-unn"
   },
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "hUN0DJQo-vmf"
   },
   "outputs": [],
   "source": [
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GczR7J4u6L5Q",
    "outputId": "15259e58-4468-44ff-dcf3-6c8dfb86d6bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 24 10:53:10 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A10          On   | 00000000:06:00.0 Off |                    0 |\n",
      "|  0%   51C    P0    63W / 150W |   1959MiB / 23028MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    126949      C   /usr/bin/python3                 1957MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "yYr65bmbITE1"
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Why does Earth orbit the Sun?\",\n",
    "    \"Explain Venus.\",\n",
    "    \"Why is this code not working: 1+\\\"A\\\"?\",\n",
    "    \"Why is Java better than Python?\",\n",
    "    \"Why is Python better than Java?\",\n",
    "    \"What is the purpose of the main() function in C?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "o85FrKyF-wg-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 303/835 [04:57<08:42,  1.02it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "n_pad is too small, use 2056 or greater.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3fffb970ab35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbatch_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-35b6efdf9907>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-2f245e1df968>\u001b[0m in \u001b[0;36mtokenize_str\u001b[0;34m(self, sentence, encoding)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_pad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_byte\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_pad\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"n_pad is too small, use {len(base)} or greater.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: n_pad is too small, use 2056 or greater."
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "epoch_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    batch_losses = []\n",
    "    for data in tqdm.tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data = data.to(device)\n",
    "\n",
    "        output, loss = model(data, True)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "        \n",
    "    epoch_losses.append(sum(batch_losses)/len(batch_losses))\n",
    "\n",
    "    plt.plot(range(len(batch_losses)),batch_losses)\n",
    "    plt.xlabel(\"Batch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Batch loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(range(len(epoch_losses)), epoch_losses)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Epoch loss\")\n",
    "    plt.show()\n",
    "\n",
    "    torch.save(model, prefix_models+f\"model_E{epoch}\")\n",
    "\n",
    "    with open(prefix_models+\"losses.txt\", \"a\") as f:\n",
    "        f.write(f\"{epoch_losses[-1]}\\n\")\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            print(f\"Prompt {i}: {prompt}\")\n",
    "            output=generate(prompt)\n",
    "            print(f\"Model output: {output}\")\n",
    "            print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUracnfcZSLX"
   },
   "outputs": [],
   "source": [
    "model = torch.load(prefix_models+\"model_E9\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        print(f\"Prompt {i}: {prompt}\")\n",
    "        output=generate(prompt)\n",
    "        print(f\"Model output: {output}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r31WSN9YiTuZ"
   },
   "outputs": [],
   "source": [
    "import builtins\n",
    "while True:\n",
    "    prompt = builtins.input(\">>> \")\n",
    "    output=generate(prompt)\n",
    "    print(f\"Model output: {output}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
